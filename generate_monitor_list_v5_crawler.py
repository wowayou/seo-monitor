import asyncio
import os
import re
import sys
import threading
import tkinter as tk
from tkinter import filedialog, messagebox, ttk
from urllib.parse import urlparse, urljoin
import pandas as pd
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError

# ================= âš™ï¸ å…¨å±€é…ç½® =================

# å…³é”®è¯æ˜ å°„ (ä¸ v4 ä¿æŒä¸€è‡´)
KEYWORDS = {
    "Contact": ["contact", "lianxi", "è”ç³»", "support"],
    "About": ["about", "profile", "story", "guanyu", "company", "ç®€ä»‹", "å…³äº"],
    "FAQ": ["faq", "help", "question", "wenti", "å¸¸è§é—®é¢˜"],
    "News": ["news", "blog", "press", "media", "insight", "article", "zixun", "dongtai", "journal", "èµ„è®¯", "æ–°é—»", "åŠ¨æ€"],
    "Product": ["product", "item", "shop", "store", "collection", "category", "solution", "service", "chanpin", "anli", "äº§å“", "æ¡ˆä¾‹", "æœåŠ¡", "è§£å†³æ–¹æ¡ˆ"],
    "Search": ["search", "sousuo", "æœç´¢", "?s="]
}

# å¿½ç•¥çš„èµ„æºåç¼€
IGNORED_EXTENSIONS = {
    '.png', '.jpg', '.jpeg', '.gif', '.svg', '.webp', '.pdf', '.doc', '.docx', 
    '.xls', '.xlsx', '.zip', '.rar', '.mp4', '.mp3', '.css', '.js', '.json', '.xml'
}

class CrawlerConfig:
    def __init__(self):
        self.input_file = ""
        self.output_file = "urls.xlsx"
        self.check_indexability = False  # æ˜¯å¦æ£€æŸ¥å¯ç´¢å¼•æ€§
        self.max_pages_per_site = 50     # å•ä¸ªç«™ç‚¹æœ€å¤§æŠ“å–æ•° (è½¯é™åˆ¶)
        self.concurrency = 3             # å¹¶å‘ç«™ç‚¹æ•°
        self.headless = True             # æ— å¤´æ¨¡å¼

# ================= ğŸ•·ï¸ çˆ¬è™«æ ¸å¿ƒé€»è¾‘ =================

class SmartCrawler:
    def __init__(self, config, log_callback):
        self.cfg = config
        self.log = log_callback
        self.stop_signal = False

    def get_slug_identifier(self, url):
        """ä»URLè·å–å”¯ä¸€æ ‡è¯†ç¬¦(Slug)"""
        try:
            path = urlparse(str(url)).path.strip('/')
            if not path: return "home"
            slug = path.split('/')[-1]
            if (slug.isdigit() or len(slug) < 3) and '/' in path:
                slug = path.split('/')[-2] + '-' + slug
            if '.' in slug: slug = slug.rsplit('.', 1)[0]
            return slug[:30]
        except: return "unknown"

    def classify_page(self, url, title=""):
        """æ ¸å¿ƒåˆ†ç±»é€»è¾‘"""
        u = str(url).lower()
        t = str(title).lower()
        path = urlparse(u).path

        # 1. é¦–é¡µ
        if path in ["", "/", "/index.php", "/index.html", "/default.aspx"]:
            return "é¦–é¡µ", None, 100

        # 2. æœç´¢é¡µ
        if any(k in u for k in KEYWORDS["Search"]):
            return "æœç´¢é¡µ", None, 90

        # 3. å…³äºæˆ‘ä»¬
        if any(k in u or k in t for k in KEYWORDS["About"]):
            return "å…³äºæˆ‘ä»¬", None, 90

        # 4. è”ç³»æˆ‘ä»¬
        if any(k in u or k in t for k in KEYWORDS["Contact"]):
            return "è”ç³»æˆ‘ä»¬", None, 90

        # 5. FAQ
        if any(k in u or k in t for k in KEYWORDS["FAQ"]):
            return "FAQ", None, 90

        # 6. æ–°é—»/åšå®¢
        if any(k in u for k in KEYWORDS["News"]):
            is_list = False
            if "category" in u or "tag" in u or "list" in u or path.endswith("/news/") or path.endswith("/blog/"):
                is_list = True
            elif len(path.strip("/").split("/")) <= 2: 
                is_list = True
            return "æ–°é—»", "èšåˆé¡µ" if is_list else "è¯¦æƒ…é¡µ", 80

        # 7. äº§å“/è§£å†³æ–¹æ¡ˆ
        if any(k in u for k in KEYWORDS["Product"]):
            is_list = False
            if "category" in u or "collection" in u or "list" in u or path.endswith("/product/") or path.endswith("/products/"):
                is_list = True
            elif len(path.strip("/").split("/")) > 3: 
                is_list = False
            return "äº§å“", "èšåˆé¡µ" if is_list else "è¯¦æƒ…é¡µ", 80

        return "å…¶ä»–", None, 0

    async def handle_age_gate(self, page):
        """å¤„ç†å¹´é¾„éªŒè¯å¼¹çª—"""
        # å¸¸è§å¼¹çª—é€‰æ‹©å™¨
        selectors = [
            ".lay-btn .colsebtn1",   # ç”¨æˆ·æŒ‡å®šçš„
            "a.act.colsebtn1",       # å˜ä½“
            "button:has-text('21+')",
            "a:has-text('21+')",
            "button:has-text('I am 21')",
            "button:has-text('Yes')",
            "button:has-text('Enter Site')",
            "#age-gate-yes",
            ".age-gate-submit"
        ]
        
        for sel in selectors:
            try:
                if await page.locator(sel).is_visible(timeout=2000):
                    self.log(f"   ğŸ›¡ï¸ æ£€æµ‹åˆ°å¹´é¾„å¼¹çª—ï¼Œå°è¯•ç‚¹å‡»: {sel}")
                    await page.locator(sel).click()
                    await asyncio.sleep(1) # ç­‰å¾…æ¶ˆå¤±
                    return True
            except: pass
        return False

    async def is_indexable(self, page):
        """æ£€æŸ¥é¡µé¢æ˜¯å¦å¯ç´¢å¼•"""
        try:
            # 1. æ£€æŸ¥ meta robots
            meta_robots = await page.locator('meta[name="robots"]').get_attribute('content')
            if meta_robots and "noindex" in meta_robots.lower():
                return False
            
            # 2. æ£€æŸ¥ title æ˜¯å¦åŒ…å« 404
            title = await page.title()
            if "404" in title or "not found" in title.lower():
                return False
                
            return True
        except:
            return True # é»˜è®¤æ”¾è¡Œ

    async def crawl_site(self, context, start_url, project_name):
        """çˆ¬å–å•ä¸ªç«™ç‚¹"""
        domain = urlparse(start_url).netloc
        self.log(f"ğŸŒ [{project_name}] å¼€å§‹çˆ¬å–: {start_url}")
        
        discovered_links = set()
        pools = {k: [] for k in ["é¦–é¡µ", "å…³äºæˆ‘ä»¬", "è”ç³»æˆ‘ä»¬", "FAQ", "æœç´¢é¡µ", "æ–°é—»èšåˆé¡µ", "æ–°é—»è¯¦æƒ…é¡µ", "äº§å“èšåˆé¡µ", "äº§å“è¯¦æƒ…é¡µ", "äº§å“åˆ†ç±»é¡µ"]}
        
        page = await context.new_page()
        
        try:
            # 1. è®¿é—®é¦–é¡µ
            try:
                await page.goto(start_url, timeout=40000, wait_until="domcontentloaded")
            except:
                self.log(f"âš ï¸ [{project_name}] é¦–é¡µè®¿é—®å¤±è´¥ï¼Œé‡è¯•...")
                await page.goto(start_url, timeout=60000, wait_until="load")

            # 2. å¤„ç†å¼¹çª—
            await self.handle_age_gate(page)
            
            # 3. æ»šåŠ¨åŠ è½½
            for _ in range(3):
                await page.mouse.wheel(0, 1000)
                await asyncio.sleep(0.5)

            # 4. è·å–é¦–é¡µæ‰€æœ‰é“¾æ¥
            hrefs = await page.evaluate("""() => {
                return Array.from(document.querySelectorAll('a')).map(a => a.href)
            }""")
            
            # 5. åˆæ­¥ç­›é€‰ä¸åˆ†ç±»
            internal_links = []
            for href in hrefs:
                u = urlparse(href)
                # å¿…é¡»æ˜¯åŒåŸŸå
                if u.netloc == domain or not u.netloc:
                    # æ’é™¤é™æ€èµ„æº
                    path = u.path.lower()
                    if any(path.endswith(ext) for ext in IGNORED_EXTENSIONS): continue
                    
                    full_url = urljoin(start_url, href)
                    full_url = full_url.split('#')[0].rstrip('/') # å»é‡hashå’Œæœ«å°¾æ–œæ 
                    
                    if full_url not in discovered_links and full_url.startswith("http"):
                        discovered_links.add(full_url)
                        internal_links.append(full_url)
                        
                        # ç«‹å³åˆ†ç±»
                        cat, sub, _ = self.classify_page(full_url)
                        if cat == "é¦–é¡µ": pools["é¦–é¡µ"].append(full_url)
                        elif cat == "å…³äºæˆ‘ä»¬": pools["å…³äºæˆ‘ä»¬"].append(full_url)
                        elif cat == "è”ç³»æˆ‘ä»¬": pools["è”ç³»æˆ‘ä»¬"].append(full_url)
                        elif cat == "FAQ": pools["FAQ"].append(full_url)
                        elif cat == "æœç´¢é¡µ": pools["æœç´¢é¡µ"].append(full_url)
                        elif cat == "æ–°é—»":
                            if sub == "èšåˆé¡µ": pools["æ–°é—»èšåˆé¡µ"].append(full_url)
                            else: pools["æ–°é—»è¯¦æƒ…é¡µ"].append(full_url)
                        elif cat == "äº§å“":
                            if sub == "èšåˆé¡µ": 
                                if "category" in full_url: pools["äº§å“åˆ†ç±»é¡µ"].append(full_url)
                                else: pools["äº§å“èšåˆé¡µ"].append(full_url)
                            else: pools["äº§å“è¯¦æƒ…é¡µ"].append(full_url)

            self.log(f"   ğŸ“Š [{project_name}] é¦–é¡µå‘ç° {len(internal_links)} ä¸ªé“¾æ¥")

            # 6. äºŒçº§æ·±åº¦æœç´¢ (å¦‚æœç¼ºå°‘å…³é”®é¡µé¢)
            # ç­–ç•¥ï¼šå¦‚æœç¼ºå°‘è¯¦æƒ…é¡µï¼Œä½†æœ‰èšåˆé¡µï¼Œå»èšåˆé¡µæŠ“å–
            
            async def quick_fetch_children(parent_url):
                self.log(f"   ğŸ” [{project_name}] æ·±å…¥æŠ“å–: {parent_url}")
                try:
                    await page.goto(parent_url, timeout=30000, wait_until="domcontentloaded")
                    await self.handle_age_gate(page)
                    child_hrefs = await page.evaluate("""() => Array.from(document.querySelectorAll('a')).map(a => a.href)""")
                    new_found = 0
                    for h in child_hrefs:
                        fu = urljoin(start_url, h).split('#')[0].rstrip('/')
                        if fu not in discovered_links and domain in fu:
                             discovered_links.add(fu)
                             cat, sub, _ = self.classify_page(fu)
                             if cat == "äº§å“" and sub == "è¯¦æƒ…é¡µ": pools["äº§å“è¯¦æƒ…é¡µ"].append(fu)
                             elif cat == "æ–°é—»" and sub == "è¯¦æƒ…é¡µ": pools["æ–°é—»è¯¦æƒ…é¡µ"].append(fu)
                             new_found += 1
                    return new_found
                except: return 0

            # è¡¥å…¨äº§å“è¯¦æƒ…
            if not pools["äº§å“è¯¦æƒ…é¡µ"] and (pools["äº§å“èšåˆé¡µ"] or pools["äº§å“åˆ†ç±»é¡µ"]):
                candidates = pools["äº§å“åˆ†ç±»é¡µ"] + pools["äº§å“èšåˆé¡µ"]
                # é€‰æœ€çŸ­çš„ä¸€ä¸ªå»æŠ“
                if candidates:
                    target = sorted(candidates, key=len)[0]
                    await quick_fetch_children(target)

            # è¡¥å…¨æ–°é—»è¯¦æƒ…
            if not pools["æ–°é—»è¯¦æƒ…é¡µ"] and pools["æ–°é—»èšåˆé¡µ"]:
                target = sorted(pools["æ–°é—»èšåˆé¡µ"], key=len)[0]
                await quick_fetch_children(target)

            # 7. ç”Ÿæˆå€™é€‰åˆ—è¡¨ (Selection)
            final_candidates = []
            
            # è¾…åŠ©å‡½æ•°ï¼šæ·»åŠ å€™é€‰
            def add_candidate(pool_key, cat_name, type_name_tmpl, selection_strategy="shortest", limit=1):
                if not pools[pool_key]: return
                
                # æ’åºç­–ç•¥
                if selection_strategy == "shortest":
                    sorted_list = sorted(list(set(pools[pool_key])), key=lambda x: (len(x), x))
                else: # longest / median
                    sorted_list = sorted(list(set(pools[pool_key])), key=lambda x: (len(x), x))
                
                selected = []
                if selection_strategy == "median" and len(sorted_list) > 2:
                    mid = len(sorted_list) // 2
                    selected = [sorted_list[mid]]
                elif selection_strategy == "longest":
                    selected = [sorted_list[-1]]
                else: # shortest
                    selected = sorted_list[:limit]
                
                for url in selected:
                    # ç¡®å®š PageType åç§°
                    if "å•é¡µ" in type_name_tmpl or "åˆ†ç±»é¡µ" in type_name_tmpl:
                        slug = self.get_slug_identifier(url)
                        p_type = f"{type_name_tmpl}-{slug}"
                    else:
                        p_type = type_name_tmpl
                        
                    final_candidates.append({
                        "Project": project_name,
                        "Category": cat_name,
                        "PageType": p_type,
                        "URL": url
                    })

            # æ‰§è¡Œç­›é€‰
            if pools["é¦–é¡µ"]: 
                final_candidates.append({"Project": project_name, "Category": "é¦–é¡µ", "PageType": "é¦–é¡µ", "URL": pools["é¦–é¡µ"][0]})
            else:
                final_candidates.append({"Project": project_name, "Category": "é¦–é¡µ", "PageType": "é¦–é¡µ", "URL": start_url})

            add_candidate("å…³äºæˆ‘ä»¬", "å…³äºæˆ‘ä»¬", "å…³äºæˆ‘ä»¬")
            add_candidate("è”ç³»æˆ‘ä»¬", "è”ç³»æˆ‘ä»¬", "è”ç³»æˆ‘ä»¬")
            add_candidate("FAQ", "FAQ", "FAQ")
            add_candidate("æœç´¢é¡µ", "æœç´¢é¡µ", "æœç´¢é¡µ")
            
            add_candidate("æ–°é—»èšåˆé¡µ", "æ–°é—»", "æ–°é—»èšåˆé¡µ")
            add_candidate("æ–°é—»è¯¦æƒ…é¡µ", "æ–°é—»", "æ–°é—»å•é¡µ", selection_strategy="longest") # è¯¦æƒ…é¡µé€šå¸¸é•¿
            
            add_candidate("äº§å“èšåˆé¡µ", "äº§å“", "äº§å“èšåˆé¡µ")
            add_candidate("äº§å“åˆ†ç±»é¡µ", "äº§å“", "äº§å“åˆ†ç±»é¡µ")
            add_candidate("äº§å“è¯¦æƒ…é¡µ", "äº§å“", "äº§å“å•é¡µ", selection_strategy="median") # é€‰ä¸­ç­‰é•¿åº¦çš„

            # --- NEW: Check SEO Core Files ---
            self.log(f"   ğŸ¤– [{project_name}] æ£€æŸ¥ SEO æ ¸å¿ƒæ–‡ä»¶...")
            
            # 1. æ£€æŸ¥ robots.txt
            robots_url = urljoin(start_url, "/robots.txt")
            robots_content = ""
            try:
                resp_robots = await page.request.get(robots_url)
                if resp_robots.status == 200:
                    self.log(f"      âœ… å‘ç° Robots.txt: {robots_url}")
                    final_candidates.append({
                        "Project": project_name,
                        "Category": "SEOæ ¸å¿ƒ",
                        "PageType": "Robots.txt",
                        "URL": robots_url
                    })
                    # å°è¯•è·å–å†…å®¹ä»¥è§£æ Sitemap
                    try:
                        robots_content = await resp_robots.text()
                    except: pass
                else:
                    self.log(f"      âš ï¸ æœªæ‰¾åˆ° Robots.txt (Status: {resp_robots.status})")
            except Exception as e:
                self.log(f"      âŒ æ£€æŸ¥ Robots.txt å‡ºé”™: {e}")

            # 2. æ£€æŸ¥ Sitemap
            sitemap_found = False
            sitemap_candidates = []
            
            # 2.1 ä» robots.txt è§£æ (ä¼˜å…ˆçº§æœ€é«˜)
            if robots_content:
                found_in_robots = re.findall(r'Sitemap:\s*(http[s]?://[^\s]+)', robots_content, re.IGNORECASE)
                for sm in found_in_robots:
                    sitemap_candidates.append(sm.strip())
            
            # 2.2 æ·»åŠ å¸¸è§è·¯å¾„å˜ä½“
            common_paths = [
                "/sitemap.xml",
                "/sitemap_index.xml", 
                "/sitemap-index.xml",
                "/wp-sitemap.xml",
                "/sitemap/sitemap.xml"
            ]
            for p in common_paths:
                sitemap_candidates.append(urljoin(start_url, p))
            
            # å»é‡å¹¶ä¿æŒé¡ºåº
            unique_candidates = []
            for c in sitemap_candidates:
                if c not in unique_candidates: unique_candidates.append(c)
                
            # 2.3 ä¾æ¬¡æ¢æµ‹
            self.log(f"      ğŸ” å¼€å§‹æ¢æµ‹ Sitemap (å…± {len(unique_candidates)} ä¸ªæ½œåœ¨è·¯å¾„)...")
            for sm_url in unique_candidates:
                try:
                    resp_sm = await page.request.get(sm_url)
                    if resp_sm.status == 200:
                        self.log(f"      âœ… å‘ç° Sitemap: {sm_url}")
                        final_candidates.append({
                            "Project": project_name,
                            "Category": "SEOæ ¸å¿ƒ",
                            "PageType": "Sitemap",
                            "URL": sm_url
                        })
                        sitemap_found = True
                        break # æ‰¾åˆ°ä¸€ä¸ªèƒ½ç”¨çš„å°±è¡Œï¼Œé¿å…é‡å¤æ·»åŠ å¹²æ‰°ç›‘æ§
                except: pass
            
            if not sitemap_found:
                self.log(f"      âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„ Sitemap! (å·²å°è¯• {len(unique_candidates)} ä¸ªè·¯å¾„)")
                # å³ä½¿æ²¡æ‰¾åˆ°ï¼Œä¹Ÿå¯ä»¥æŠŠ sitemap.xml ä½œä¸ºå ä½ç¬¦åŠ è¿›å»ï¼Œæˆ–è€…å°±ä¸åŠ äº†ä»¥å…ç›‘æ§æŠ¥é”™ï¼Ÿ
                # ç”¨æˆ·è¦æ±‚"é˜²æ­¢é™é»˜å¤±è´¥"ï¼Œè¿™é‡Œå·²ç»æ‰“å°äº†è­¦å‘Šæ—¥å¿—ã€‚
                # ä¹Ÿå¯ä»¥æ·»åŠ ä¸€ä¸ª "Sitemap-Missing" çš„æ¡ç›®ï¼Ÿæš‚æ—¶åªè®°å½•æ—¥å¿—ã€‚

            # 8. å¯ç´¢å¼•æ€§æ£€æŸ¥ (Check Indexability)
            valid_results = []
            if self.cfg.check_indexability:
                self.log(f"   ğŸ•µï¸ [{project_name}] æ­£åœ¨æ£€æŸ¥ {len(final_candidates)} ä¸ªé¡µé¢çš„å¯ç´¢å¼•æ€§...")
                for item in final_candidates:
                    if self.stop_signal: break
                    
                    # è·³è¿‡é HTML é¡µé¢çš„æ£€æŸ¥
                    if item["Category"] == "SEOæ ¸å¿ƒ":
                        valid_results.append(item)
                        continue

                    try:
                        # å¤ç”¨å½“å‰é¡µé¢å¯¹è±¡è¿›è¡Œæ£€æŸ¥
                        await page.goto(item["URL"], timeout=20000, wait_until="domcontentloaded")
                        # ä¸éœ€è¦ç­‰å¤ªä¹…ï¼Œåªè¦èƒ½çœ‹åˆ° meta å³å¯
                        is_ok = await self.is_indexable(page)
                        if is_ok:
                            valid_results.append(item)
                        else:
                            self.log(f"      ğŸš« è·³è¿‡ä¸å¯ç´¢å¼•é¡µé¢: {item['PageType']}")
                    except Exception as e:
                        # è®¿é—®å‡ºé”™ä¹Ÿç®—é€šè¿‡å§ï¼Œé˜²æ­¢è¯¯æ€
                        valid_results.append(item)
            else:
                valid_results = final_candidates

            return valid_results

        except Exception as e:
            self.log(f"âŒ [{project_name}] çˆ¬å–å¼‚å¸¸: {e}")
            # è‡³å°‘è¿”å›é¦–é¡µ
            return [{"Project": project_name, "Category": "é¦–é¡µ", "PageType": "é¦–é¡µ", "URL": start_url}]
        finally:
            await page.close()

    async def run(self):
        self.log("ğŸš€ å¯åŠ¨æ™ºèƒ½çˆ¬è™«ä»»åŠ¡...")
        
        # 1. è¯»å–è¾“å…¥
        urls = []
        try:
            if self.cfg.input_file.endswith(('.xlsx', '.xls')):
                df = pd.read_excel(self.cfg.input_file)
                # å°è¯•æ‰¾ URL åˆ—
                col = next((c for c in df.columns if 'url' in c.lower() or 'address' in c.lower()), df.columns[0])
                urls = df[col].dropna().astype(str).tolist()
            else:
                with open(self.cfg.input_file, 'r', encoding='utf-8') as f:
                    urls = [line.strip() for line in f if line.strip()]
        except Exception as e:
            self.log(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
            return

        self.log(f"ğŸ“‚ è¯»å–åˆ° {len(urls)} ä¸ªç›®æ ‡ç«™ç‚¹")

        all_results = []
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=self.cfg.headless)
            
            # é™åˆ¶å¹¶å‘
            semaphore = asyncio.Semaphore(self.cfg.concurrency)
            
            async def worker(url):
                if self.stop_signal: return
                async with semaphore:
                    # æå–é¡¹ç›®å
                    parsed = urlparse(url)
                    if not parsed.scheme: url = "https://" + url
                    domain = urlparse(url).netloc.replace("www.", "")
                    project_name = domain.split('.')[0].capitalize()
                    
                    context = await browser.new_context(
                        user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36"
                    )
                    
                    try:
                        res = await self.crawl_site(context, url, project_name)
                        all_results.extend(res)
                    finally:
                        await context.close()

            tasks = [worker(u) for u in urls]
            await asyncio.gather(*tasks)
            await browser.close()

        if self.stop_signal:
            self.log("ğŸ›‘ ä»»åŠ¡å·²åœæ­¢")
        
        # å¯¼å‡ºç»“æœ
        if all_results:
            df_out = pd.DataFrame(all_results)
            # æ’åºï¼šProject -> Category
            df_out.sort_values(by=['Project', 'Category'], inplace=True)
            df_out = df_out[['Project', 'Category', 'PageType', 'URL']]
            
            df_out.to_excel(self.cfg.output_file, index=False)
            self.log(f"\nâœ¨ ä»»åŠ¡å®Œæˆï¼ç”Ÿæˆç»“æœ: {self.cfg.output_file}")
            self.log(f"ğŸ“Š æ€»è®¡è·å– {len(df_out)} æ¡ç›‘æ§è§„åˆ™")
            try:
                os.startfile(self.cfg.output_file)
            except: pass
        else:
            self.log("âš ï¸ æœªè·å–åˆ°ä»»ä½•æœ‰æ•ˆæ•°æ®")

# ================= ğŸ–¥ï¸ GUI ç•Œé¢ =================

class CrawlerApp:
    def __init__(self, root):
        self.root = root
        self.root.title("SEO æ™ºèƒ½URLè·å–å·¥å…· v5.0 (Crawlerç‰ˆ)")
        self.root.geometry("600x550")
        
        self.input_path = tk.StringVar()
        self.check_idx = tk.BooleanVar(value=True) # é»˜è®¤å¼€å¯ç´¢å¼•æ£€æŸ¥
        self.headless_mode = tk.BooleanVar(value=True)
        
        self.crawler = None
        self._create_widgets()

    def _create_widgets(self):
        main_frame = ttk.Frame(self.root, padding="20")
        main_frame.pack(fill=tk.BOTH, expand=True)
        
        # æ ‡é¢˜
        ttk.Label(main_frame, text="ğŸ•·ï¸ ç½‘ç«™URLæ™ºèƒ½æŠ“å–ç”Ÿæˆå™¨", font=('Microsoft YaHei', 14, 'bold')).pack(pady=(0, 20))
        
        # 1. è¾“å…¥æ–‡ä»¶
        frame1 = ttk.LabelFrame(main_frame, text="1. è¾“å…¥æ–‡ä»¶ (Txt/Excel - ä»…å«é¦–é¡µURL)", padding=10)
        frame1.pack(fill=tk.X, pady=5)
        ttk.Entry(frame1, textvariable=self.input_path).pack(side=tk.LEFT, fill=tk.X, expand=True, padx=(0, 5))
        ttk.Button(frame1, text="æµè§ˆ...", command=self.browse_input).pack(side=tk.RIGHT)
        
        # 2. é€‰é¡¹
        frame2 = ttk.LabelFrame(main_frame, text="2. æŠ“å–é€‰é¡¹", padding=10)
        frame2.pack(fill=tk.X, pady=5)
        
        ttk.Checkbutton(frame2, text="ä»…ç­›é€‰å¯ç´¢å¼•é¡µé¢ (Check Indexable)", variable=self.check_idx).grid(row=0, column=0, sticky=tk.W, padx=10)
        ttk.Label(frame2, text="â„¹ï¸ å¼€å¯åä¼šè‡ªåŠ¨è¿‡æ»¤ noindex å’Œ 404 é¡µé¢ï¼Œä½†é€Ÿåº¦ä¼šå˜æ…¢").grid(row=1, column=0, sticky=tk.W, padx=10, pady=(2,0))
        
        ttk.Checkbutton(frame2, text="åå°é™é»˜è¿è¡Œ (Headless)", variable=self.headless_mode).grid(row=2, column=0, sticky=tk.W, padx=10, pady=(10,0))
        
        # 3. æ—¥å¿—
        ttk.Label(main_frame, text="è¿è¡Œæ—¥å¿—:").pack(anchor=tk.W, pady=(10, 0))
        self.log_text = tk.Text(main_frame, height=12, font=('Consolas', 9), state='disabled')
        self.log_text.pack(fill=tk.BOTH, expand=True, pady=5)
        
        # æŒ‰é’®
        btn_frame = ttk.Frame(main_frame)
        btn_frame.pack(fill=tk.X, pady=10)
        self.start_btn = ttk.Button(btn_frame, text="å¼€å§‹æŠ“å–", command=self.start)
        self.start_btn.pack(side=tk.LEFT, fill=tk.X, expand=True, padx=5)
        
        ttk.Button(btn_frame, text="åœæ­¢", command=self.stop).pack(side=tk.RIGHT, fill=tk.X, expand=True, padx=5)

    def browse_input(self):
        f = filedialog.askopenfilename(filetypes=[("Data Files", "*.txt;*.xlsx;*.xls")])
        if f: self.input_path.set(f)

    def log(self, msg):
        def _update():
            self.log_text.config(state='normal')
            self.log_text.insert(tk.END, str(msg) + "\n")
            self.log_text.see(tk.END)
            self.log_text.config(state='disabled')
        self.root.after(0, _update)

    def start(self):
        if not self.input_path.get():
            messagebox.showerror("é”™è¯¯", "è¯·å…ˆé€‰æ‹©è¾“å…¥æ–‡ä»¶ï¼")
            return
            
        self.start_btn.config(state='disabled')
        
        cfg = CrawlerConfig()
        cfg.input_file = self.input_path.get()
        cfg.check_indexability = self.check_idx.get()
        cfg.headless = self.headless_mode.get()
        
        self.crawler = SmartCrawler(cfg, self.log)
        
        thread = threading.Thread(target=self.run_async, args=(self.crawler,), daemon=True)
        thread.start()

    def stop(self):
        if self.crawler:
            self.crawler.stop_signal = True
            self.log("ğŸ›‘ æ­£åœ¨åœæ­¢...")

    def run_async(self, crawler):
        asyncio.run(crawler.run())
        self.root.after(0, lambda: self.start_btn.config(state='normal'))

def main():
    root = tk.Tk()
    app = CrawlerApp(root)
    root.mainloop()

if __name__ == "__main__":
    main()
