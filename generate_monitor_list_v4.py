import pandas as pd
import os
import re
from urllib.parse import urlparse
import tkinter as tk
from tkinter import filedialog

# --- é…ç½® ---
DEFAULT_INPUT_FILE = "crawl_result.xlsx"
OUTPUT_FILE = "urls.xlsx"

# å…³é”®è¯æ˜ å°„ (å¯æ ¹æ®éœ€æ±‚æ‰©å±•)
KEYWORDS = {
    "Contact": ["contact", "lianxi", "è”ç³»", "support"],
    "About": ["about", "profile", "story", "guanyu", "company", "ç®€ä»‹", "å…³äº"],
    "FAQ": ["faq", "help", "question", "wenti", "å¸¸è§é—®é¢˜"],
    "News": ["news", "blog", "press", "media", "insight", "article", "zixun", "dongtai", "journal", "èµ„è®¯", "æ–°é—»", "åŠ¨æ€"],
    "Product": ["product", "item", "shop", "store", "collection", "category", "solution", "service", "chanpin", "anli", "äº§å“", "æ¡ˆä¾‹", "æœåŠ¡", "è§£å†³æ–¹æ¡ˆ"],
    "Search": ["search", "sousuo", "æœç´¢", "?s="]
}

def select_file():
    """å¼¹å‡ºæ–‡ä»¶é€‰æ‹©æ¡†"""
    root = tk.Tk()
    root.withdraw()
    file_path = filedialog.askopenfilename(
        title="é€‰æ‹© Screaming Frog å¯¼å‡ºçš„ Excel/CSV æ–‡ä»¶",
        filetypes=[("Excel Files", "*.xlsx;*.xls"), ("CSV Files", "*.csv")]
    )
    return file_path

def get_domain_project(url, title=None):
    """ä»URLæå–é¡¹ç›®åï¼Œå°è¯•ä»æ ‡é¢˜æå–å“ç‰Œå"""
    try:
        parsed = urlparse(str(url))
        domain = parsed.netloc
        if domain.startswith("www."):
            domain = domain[4:]
        
        # å°è¯•ä»æ ‡é¢˜æå–å“ç‰Œ (é€šå¸¸åœ¨ - æˆ– | ä¹‹å)
        project_name = domain
        if title and isinstance(title, str):
            separators = ['-', '|', '_', 'â€”']
            for sep in separators:
                if sep in title:
                    candidate = title.split(sep)[-1].strip()
                    # å“ç‰Œåé€šå¸¸ä¸é•¿
                    if 1 < len(candidate) < 20:
                        project_name = candidate
                        break
        return project_name
    except:
        return "Unknown"

def classify_page(url, title, h1):
    """
    æ ¸å¿ƒåˆ†ç±»é€»è¾‘
    è¿”å›: (Category, SubType, Score)
    Category: Product, News, About, Contact, Home, Other
    SubType: List, Detail, None
    """
    u = str(url).lower()
    t = str(title).lower() if pd.notna(title) else ""
    h = str(h1).lower() if pd.notna(h1) else ""
    path = urlparse(u).path

    # 1. é¦–é¡µ
    if path in ["", "/", "/index.php", "/index.html", "/default.aspx"]:
        return "é¦–é¡µ", None, 100

    # 2. æœç´¢é¡µ
    if any(k in u for k in KEYWORDS["Search"]):
        return "æœç´¢é¡µ", None, 90

    # 3. å…³äºæˆ‘ä»¬
    if any(k in u or k in t for k in KEYWORDS["About"]):
        return "å…³äºæˆ‘ä»¬", None, 90

    # 4. è”ç³»æˆ‘ä»¬
    if any(k in u or k in t for k in KEYWORDS["Contact"]):
        return "è”ç³»æˆ‘ä»¬", None, 90

    # 5. FAQ
    if any(k in u or k in t for k in KEYWORDS["FAQ"]):
        return "FAQ", None, 90

    # 6. æ–°é—»/åšå®¢
    if any(k in u for k in KEYWORDS["News"]):
        # åˆ¤æ–­æ˜¯åˆ—è¡¨è¿˜æ˜¯è¯¦æƒ…
        # åˆ—è¡¨ç‰¹å¾: è·¯å¾„çŸ­, åŒ…å« category, tag, list
        # è¯¦æƒ…ç‰¹å¾: è·¯å¾„é•¿, åŒ…å« .html, æ—¥æœŸ, å…·ä½“æ–‡ç« å
        
        is_list = False
        if "category" in u or "tag" in u or "list" in u or path.endswith("/news/") or path.endswith("/blog/"):
            is_list = True
        elif len(path.strip("/").split("/")) <= 2: # è·¯å¾„å¾ˆæµ…å¯èƒ½æ˜¯åˆ—è¡¨
            is_list = True
            
        return "æ–°é—»", "èšåˆé¡µ" if is_list else "è¯¦æƒ…é¡µ", 80

    # 7. äº§å“/è§£å†³æ–¹æ¡ˆ
    if any(k in u for k in KEYWORDS["Product"]):
        is_list = False
        if "category" in u or "collection" in u or "list" in u or path.endswith("/product/") or path.endswith("/products/"):
            is_list = True
        # æ’é™¤å¯èƒ½æ˜¯è¯¦æƒ…çš„æƒ…å†µ
        elif len(path.strip("/").split("/")) > 3: 
            is_list = False
            
        return "äº§å“", "èšåˆé¡µ" if is_list else "è¯¦æƒ…é¡µ", 80

    return "å…¶ä»–", None, 0

def get_slug_identifier(url):
    """ä»URLè·å–å”¯ä¸€æ ‡è¯†ç¬¦(Slug)ï¼Œç”¨äºç”Ÿæˆç¨³å®šçš„æ–‡ä»¶å"""
    try:
        path = urlparse(str(url)).path.strip('/')
        if not path: return "home"
        
        # è·å–æœ€åä¸€æ®µ
        slug = path.split('/')[-1]
        
        # å¦‚æœæœ€åä¸€æ®µæ˜¯æ•°å­—æˆ–å¤ªçŸ­ï¼Œå–å‰ä¸€æ®µ
        if (slug.isdigit() or len(slug) < 3) and '/' in path:
             slug = path.split('/')[-2] + '-' + slug
             
        # å»é™¤æ‰©å±•å
        if '.' in slug:
            slug = slug.rsplit('.', 1)[0]
            
        return slug[:30] # é™åˆ¶é•¿åº¦
    except:
        return "unknown"

def main():
    print("ğŸš€ å¯åŠ¨ URL æ™ºèƒ½åˆ†ç±»å·¥å…·...")
    
    # 1. è·å–æ–‡ä»¶
    input_file = select_file()
    if not input_file:
        print("âŒ æœªé€‰æ‹©æ–‡ä»¶ï¼Œç¨‹åºé€€å‡º")
        return

    print(f"ğŸ“‚ æ­£åœ¨è¯»å–: {input_file}")
    
    try:
        if input_file.endswith('.csv'):
            df = pd.read_csv(input_file)
        else:
            df = pd.read_excel(input_file)
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return

    # 2. è§„èŒƒåŒ–åˆ—å
    df.columns = df.columns.str.strip()
    
    # å¯»æ‰¾å…³é”®åˆ—
    url_col = next((c for c in ['Address', 'URL', 'Original Url'] if c in df.columns), None)
    status_col = next((c for c in ['Status Code', 'Status'] if c in df.columns), None)
    title_col = next((c for c in ['Title 1', 'Title'] if c in df.columns), None)
    h1_col = next((c for c in ['H1-1', 'H1'] if c in df.columns), None)
    content_type_col = next((c for c in ['Content Type'] if c in df.columns), None)

    if not url_col:
        print("âŒ æ— æ³•æ‰¾åˆ° URL åˆ— (Address/URL)")
        return

    print(f"âœ… æ‰¾åˆ°å…³é”®åˆ—: URL='{url_col}', Title='{title_col}', Status='{status_col}'")

    # 3. é¢„å¤„ç†
    # è¿‡æ»¤é 200
    if status_col:
        df = df[df[status_col] == 200]
    
    # è¿‡æ»¤é HTML
    if content_type_col:
        df = df[df[content_type_col].astype(str).str.contains("html", case=False, na=False)]

    # æå–é¡¹ç›®å (åŸºäºåŸŸå)
    df['Domain_Project'] = df[url_col].apply(lambda x: get_domain_project(x))
    
    # è¿›ä¸€æ­¥ä¼˜åŒ–é¡¹ç›®åï¼šå¦‚æœåŒä¸€åŸŸåä¸‹ Title åç¼€ä¸€è‡´ï¼Œåˆ™ä½¿ç”¨ Title åç¼€
    # è¿™é‡Œç®€å•å¤„ç†ï¼šç›´æ¥ä½¿ç”¨ apply ç»“åˆ title
    if title_col:
        df['Project_Name'] = df.apply(lambda row: get_domain_project(row[url_col], row[title_col]), axis=1)
    else:
        df['Project_Name'] = df['Domain_Project']

    final_rows = []
    
    # 4. æŒ‰é¡¹ç›®åˆ†ç»„å¤„ç†
    grouped = df.groupby('Domain_Project') # è¿˜æ˜¯æŒ‰åŸŸååˆ†ç»„æœ€ç¨³å¦¥
    
    print(f"ğŸ” è¯†åˆ«åˆ° {len(grouped)} ä¸ªç½‘ç«™é¡¹ç›®ï¼Œå¼€å§‹åˆ†ç±»...")

    for domain, group in grouped:
        # è·å–è¯¥ç»„æœ€å¸¸ç”¨çš„ Project Name (ä¼—æ•°)
        project_name = group['Project_Name'].mode()[0] if not group['Project_Name'].empty else domain
        print(f"   - å¤„ç†: {project_name} ({domain}) | é¡µé¢æ•°: {len(group)}")
        
        # åˆ†ç±»å®¹å™¨
        pools = {
            "é¦–é¡µ": [],
            "å…³äºæˆ‘ä»¬": [],
            "è”ç³»æˆ‘ä»¬": [],
            "FAQ": [],
            "æœç´¢é¡µ": [],
            "æ–°é—»èšåˆé¡µ": [],
            "æ–°é—»è¯¦æƒ…é¡µ": [],
            "äº§å“èšåˆé¡µ": [],
            "äº§å“è¯¦æƒ…é¡µ": [],
            "äº§å“åˆ†ç±»é¡µ": [] # é¢å¤–åŒºåˆ†
        }

        for _, row in group.iterrows():
            url = row[url_col]
            title = row[title_col] if title_col else ""
            h1 = row[h1_col] if h1_col else ""
            
            cat, sub, score = classify_page(url, title, h1)
            
            if cat == "é¦–é¡µ":
                pools["é¦–é¡µ"].append(url)
            elif cat == "å…³äºæˆ‘ä»¬":
                pools["å…³äºæˆ‘ä»¬"].append(url)
            elif cat == "è”ç³»æˆ‘ä»¬":
                pools["è”ç³»æˆ‘ä»¬"].append(url)
            elif cat == "FAQ":
                pools["FAQ"].append(url)
            elif cat == "æœç´¢é¡µ":
                pools["æœç´¢é¡µ"].append(url)
            elif cat == "æ–°é—»":
                if sub == "èšåˆé¡µ": pools["æ–°é—»èšåˆé¡µ"].append(url)
                else: pools["æ–°é—»è¯¦æƒ…é¡µ"].append(url)
            elif cat == "äº§å“":
                if sub == "èšåˆé¡µ": 
                    # ç»†åˆ†ï¼šå¦‚æœURLåŒ…å« category å¯èƒ½æ˜¯åˆ†ç±»é¡µï¼Œå¦åˆ™æ˜¯æ€»èšåˆ
                    if "category" in str(url):
                        pools["äº§å“åˆ†ç±»é¡µ"].append(url)
                    else:
                        pools["äº§å“èšåˆé¡µ"].append(url)
                else: 
                    pools["äº§å“è¯¦æƒ…é¡µ"].append(url)

        # 5. æŠ½æ ·é€»è¾‘ (Selection)
        # ä½¿ç”¨ (len(x), x) æ’åºç¡®ä¿ç¡®å®šæ€§ï¼šä¼˜å…ˆçŸ­è·¯å¾„ï¼Œé•¿åº¦ç›¸åŒæ—¶æŒ‰å­—æ¯åº
        
        # é¦–é¡µ: å¿…é€‰
        if pools["é¦–é¡µ"]:
            final_rows.append({
                "Project": project_name, 
                "Category": "é¦–é¡µ",
                "PageType": "é¦–é¡µ", 
                "URL": pools["é¦–é¡µ"][0]
            })
        
        # åŠŸèƒ½é¡µ: é€‰è·¯å¾„æœ€çŸ­çš„
        for p_type in ["å…³äºæˆ‘ä»¬", "è”ç³»æˆ‘ä»¬", "FAQ", "æœç´¢é¡µ"]:
            if pools[p_type]:
                best_url = sorted(pools[p_type], key=lambda x: (len(x), x))[0]
                final_rows.append({
                    "Project": project_name, 
                    "Category": p_type,
                    "PageType": p_type, 
                    "URL": best_url
                })

        # æ–°é—»: 
        if pools["æ–°é—»èšåˆé¡µ"]:
            # æœ€çŸ­çš„ä½œä¸ºèšåˆ
            best_url = sorted(pools["æ–°é—»èšåˆé¡µ"], key=lambda x: (len(x), x))[0]
            final_rows.append({
                "Project": project_name, 
                "Category": "æ–°é—»",
                "PageType": "æ–°é—»èšåˆé¡µ", 
                "URL": best_url
            })
        
        if pools["æ–°é—»è¯¦æƒ…é¡µ"]:
            # é€‰ä¸€ä¸ªé•¿åº¦é€‚ä¸­çš„ï¼Œæˆ–è€…æœ€æ–°çš„ (å¦‚æœæœ‰æ—¥æœŸ)
            # è¿™é‡Œç®€å•é€‰æœ€é•¿çš„ï¼Œé€šå¸¸è¯¦æƒ…é¡µURLè¾ƒé•¿
            sorted_news = sorted(pools["æ–°é—»è¯¦æƒ…é¡µ"], key=lambda x: (len(x), x))
            best_detail = sorted_news[-1] if len(sorted_news) > 0 else sorted_news[0]
            
            # ç”Ÿæˆå”¯ä¸€æ ‡è¯†
            slug = get_slug_identifier(best_detail)
            final_rows.append({
                "Project": project_name, 
                "Category": "æ–°é—»",
                "PageType": f"æ–°é—»å•é¡µ-{slug}", 
                "URL": best_detail
            })

        # äº§å“:
        # 1. èšåˆé¡µ (Root)
        if pools["äº§å“èšåˆé¡µ"]:
             best_url = sorted(pools["äº§å“èšåˆé¡µ"], key=lambda x: (len(x), x))[0]
             final_rows.append({
                 "Project": project_name, 
                 "Category": "äº§å“",
                 "PageType": "äº§å“èšåˆé¡µ", 
                 "URL": best_url
             })
        
        # 2. åˆ†ç±»é¡µ (Category)
        if pools["äº§å“åˆ†ç±»é¡µ"]:
             # é€‰ä¸€ä¸ªä»£è¡¨
             best_url = sorted(pools["äº§å“åˆ†ç±»é¡µ"], key=lambda x: (len(x), x))[0]
             slug = get_slug_identifier(best_url)
             final_rows.append({
                 "Project": project_name, 
                 "Category": "äº§å“",
                 "PageType": f"äº§å“åˆ†ç±»é¡µ-{slug}", 
                 "URL": best_url
             })
        elif not pools["äº§å“èšåˆé¡µ"] and pools["äº§å“è¯¦æƒ…é¡µ"]: 
             # å¦‚æœæ²¡æœ‰èšåˆé¡µå’Œåˆ†ç±»é¡µï¼Œä½†æœ‰è¯¦æƒ…é¡µï¼Œå¯èƒ½è¯¦æƒ…é¡µçš„ä¸Šçº§å°±æ˜¯åˆ—è¡¨ï¼Œè¿™é‡Œæš‚ä¸å¤„ç†å¤æ‚åæ¨
             pass

        # 3. è¯¦æƒ…é¡µ
        if pools["äº§å“è¯¦æƒ…é¡µ"]:
            sorted_prods = sorted(pools["äº§å“è¯¦æƒ…é¡µ"], key=lambda x: (len(x), x))
            # é€‰ä¸€ä¸ªä¸­ç­‰é•¿åº¦çš„ï¼Œé¿å…é€‰ä¸­æå…¶å¤æ‚çš„å‚æ•°é¡µ
            idx = len(sorted_prods) // 2
            best_detail = sorted_prods[idx]
            
            slug = get_slug_identifier(best_detail)
            final_rows.append({
                "Project": project_name, 
                "Category": "äº§å“",
                "PageType": f"äº§å“å•é¡µ-{slug}", 
                "URL": best_detail
            })

    # 6. è¾“å‡ºç»“æœ
    result_df = pd.DataFrame(final_rows)
    
    # è°ƒæ•´åˆ—é¡ºåº
    if not result_df.empty:
        # æ–°å¢ Category åˆ—ï¼ŒPageType ä½œä¸ºå”¯ä¸€æ–‡ä»¶åæ ‡è¯†
        result_df = result_df[['Project', 'Category', 'PageType', 'URL']]
        result_df.to_excel(OUTPUT_FILE, index=False)
        print(f"\nâœ¨ æˆåŠŸå¤„ç†ï¼å·²ç”Ÿæˆæ–‡ä»¶: {OUTPUT_FILE}")
        print(f"ğŸ“Š æ€»è®¡ç”Ÿæˆ {len(result_df)} æ¡ç›‘æ§è§„åˆ™")
        os.startfile(OUTPUT_FILE)
    else:
        print("âš ï¸ æœªåŒ¹é…åˆ°ä»»ä½•æœ‰æ•ˆé¡µé¢ï¼Œè¯·æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ•°æ®ã€‚")

if __name__ == "__main__":
    main()